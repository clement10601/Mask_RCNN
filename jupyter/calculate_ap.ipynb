{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco lib path:/home/john/Mask_RCNN/coco.py\n",
      "!!! Loading weights of model !!!\n",
      "start load coco model /data1/coco/2017\n",
      "loading annotations into memory...\n",
      "Done (t=0.95s)\n",
      "creating index...\n",
      "index created!\n",
      "Image Count: 4952\n",
      "Class Count: 81\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "sys.path.append('/home/john/Mask_RCNN/')\n",
    "sys.path.append('/home/john/Mask_RCNN/maskrcnn/')\n",
    "sys.path.append('/home/john/Mask_RCNN/cocoapi/PythonAPI/')\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "#%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import skimage.io\n",
    "from maskrcnn import model as modellib\n",
    "from maskrcnn import visualize\n",
    "from model import logf\n",
    "\n",
    "# MS COCO Dataset\n",
    "import coco\n",
    "print('coco lib path:{}'.format(coco.__file__))\n",
    "\n",
    "# Coco Dataset Config\n",
    "config = coco.CocoConfig()\n",
    "\n",
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "\n",
    "# define path of coco dataset\n",
    "COCO_DIR = \"/data1/coco/2017\"\n",
    "\n",
    "### Setup Model\n",
    "# Root directory of the project\n",
    "ROOT_DIR = \"/home/john/Mask_RCNN\"\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_DIR = os.path.join(ROOT_DIR, \"weights\")\n",
    "if not os.path.exists(COCO_MODEL_DIR):\n",
    "        os.makedirs(COCO_MODEL_DIR)\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"weights/mask_rcnn_coco.h5\")\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "        utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "print(\"!!! Loading weights of model !!!\")\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
    "\n",
    "\n",
    "### Load Dataset\n",
    "if config.NAME == 'shapes':\n",
    "    dataset = shapes.ShapesDataset()\n",
    "    dataset.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "elif config.NAME == \"coco\":\n",
    "    dataset = coco.CocoDataset()\n",
    "    print('start load coco model {}'.format(COCO_DIR))\n",
    "    dataset.load_coco(COCO_DIR, \"val\")\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "dataset.prepare()\n",
    "print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset.num_classes))\n",
    "\n",
    "\"\"\"\n",
    "for i, info in enumerate(dataset.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))\n",
    "\"\"\"\n",
    "\n",
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax\n",
    "\n",
    "\n",
    "def get_unique_id_list(input_list):\n",
    "    \"\"\"\n",
    "    get unique id in input list,\n",
    "    filter duplicate item, return >=0 interger\n",
    "    \"\"\"\n",
    "    rtn_list = []\n",
    "\n",
    "    for i in input_list:\n",
    "        if i not in rtn_list and i >= 0:\n",
    "            rtn_list.append(i)\n",
    "    return rtn_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.image_id len 4952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimage_ids = np.random.choice(dataset.image_ids, 10)\\n\\nTP_total = np.zeros(dataset.num_classes)\\nFP_total = np.zeros(dataset.num_classes)\\nFN_total = np.zeros(dataset.num_classes)\\n\\nfor image_id in image_ids:\\n    print(\\'----------------------------------\\')\\n    image = dataset.load_image(image_id)\\n   \\n    # Parse image additional info           \\n    image, image_meta, gt_class_id, gt_bbox, gt_mask =    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\\n    info = dataset.image_info[image_id]\\n    print(\"Processing image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \\n                                       dataset.image_reference(image_id)))\\n    \\n    # Detect\\n    results = model.detect([image], verbose=0)\\n    ax = get_ax(1)\\n    r = results[0]\\n    \\n    # Calculate each object accuracy\\n    \\n    AP, precisions, recalls, overlaps = utils.compute_ap(gt_bbox, gt_class_id, \\n                                          r[\\'rois\\'], r[\\'class_ids\\'], r[\\'scores\\'])\\n    print(\"score flag False, AP =====> \", AP)\\n    print(\"gt_class_id \", gt_class_id)\\n    print(\"precisions\", precisions)\\n    print(\"recalls\", recalls)\\n    \\n    TP, FP, FN = utils.compute_class_PR(gt_bbox, gt_class_id,\\n                                        r[\\'rois\\'], r[\\'class_ids\\'], r[\\'scores\\'],\\n                                        dataset.num_classes, 0.51)\\n\\n    #print(\"gt_class_id\", gt_class_id)\\n    #print(\"pred_class_id\", r[\\'class_ids\\'])\\n    #print(\"TP list\", TP)\\n    #print(\"FP list \", FP)\\n    #print(\"FN list\", FN)\\n    TP_total = np.add(TP_total, TP)\\n    FP_total = np.add(FP_total, FP)\\n    FN_total = np.add(FN_total, FN)\\n\\n# Calculate mean Precision and Recall of each class\\nprint(\"TP_total=>\", TP_total)\\nprint(\"FP_total=>\", FP_total)\\nprint(\"FN_total=>\", FN_total)\\n\\nTP_add_FP = np.add(TP_total, FP_total)\\nTP_add_FN = np.add(TP_total, FN_total)\\n\\nPrecision_AVG = np.divide(TP_total, TP_add_FP, out=np.zeros_like(TP_total), where=TP_add_FP!=0)\\nprint(\"Average Precision =>\", Precision_AVG)\\n\\nRecall_AVG = np.divide(TP_total, TP_add_FN, out=np.zeros_like(TP_total), where=TP_add_FN!=0)\\nprint(\"Average Recall =>\", Recall_AVG)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "print(\"dataset.image_id len=>\", len(dataset.image_ids))\n",
    "\n",
    "#image_ids = np.random.choice(dataset.image_ids, 10)\n",
    "\n",
    "TP_total = np.zeros(dataset.num_classes)\n",
    "FP_total = np.zeros(dataset.num_classes)\n",
    "FN_total = np.zeros(dataset.num_classes)\n",
    "\n",
    "for image_id in dataset.image_ids:\n",
    "    print('----------------------------------')\n",
    "    image = dataset.load_image(image_id)\n",
    "   \n",
    "    # Parse image additional info           \n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "    info = dataset.image_info[image_id]\n",
    "    print(\"Processing image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n",
    "                                       dataset.image_reference(image_id)))\n",
    "    \n",
    "    # Detect\n",
    "    results = model.detect([image], verbose=0)\n",
    "    ax = get_ax(1)\n",
    "    r = results[0]\n",
    "    \n",
    "    # Calculate each object accuracy\n",
    "    \n",
    "    AP, precisions, recalls, overlaps = utils.compute_ap(gt_bbox, gt_class_id, \n",
    "                                          r['rois'], r['class_ids'], r['scores'])\n",
    "    print(\"score flag False, AP =====> \", AP)\n",
    "    print(\"gt_class_id \", gt_class_id)\n",
    "    print(\"precisions\", precisions)\n",
    "    print(\"recalls\", recalls)\n",
    "    \n",
    "    TP, FP, FN = utils.compute_class_PR(gt_bbox, gt_class_id,\n",
    "                                        r['rois'], r['class_ids'], r['scores'],\n",
    "                                        dataset.num_classes, 0.51)\n",
    "\n",
    "    #print(\"gt_class_id\", gt_class_id)\n",
    "    #print(\"pred_class_id\", r['class_ids'])\n",
    "    #print(\"TP list\", TP)\n",
    "    #print(\"FP list \", FP)\n",
    "    #print(\"FN list\", FN)\n",
    "    TP_total = np.add(TP_total, TP)\n",
    "    FP_total = np.add(FP_total, FP)\n",
    "    FN_total = np.add(FN_total, FN)\n",
    "\n",
    "# Calculate mean Precision and Recall of each class\n",
    "print(\"TP_total=>\", TP_total)\n",
    "print(\"FP_total=>\", FP_total)\n",
    "print(\"FN_total=>\", FN_total)\n",
    "\n",
    "TP_add_FP = np.add(TP_total, FP_total)\n",
    "TP_add_FN = np.add(TP_total, FN_total)\n",
    "\n",
    "Precision_AVG = np.divide(TP_total, TP_add_FP, out=np.zeros_like(TP_total), where=TP_add_FP!=0)\n",
    "print(\"Average Precision =>\", Precision_AVG)\n",
    "\n",
    "Recall_AVG = np.divide(TP_total, TP_add_FN, out=np.zeros_like(TP_total), where=TP_add_FN!=0)\n",
    "print(\"Average Recall =>\", Recall_AVG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish !!!\n"
     ]
    }
   ],
   "source": [
    "print(\"finish !!!\")\n",
    "#visualize.plot_precision_recall(AP, precisions, recalls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nvisualize.plot_overlaps(gt_class_id, r['class_ids'], r['scores'],\\n                        overlaps, dataset.class_names)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid of ground truth objects and their predictions\n",
    "\"\"\"\n",
    "visualize.plot_overlaps(gt_class_id, r['class_ids'], r['scores'],\n",
    "                        overlaps, dataset.class_names)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
